#!/usr/bin/env python3
import argparse
import csv
import dataclasses
import itertools
import json
import logging
import os
import subprocess
from collections import Counter
from dataclasses import dataclass, field
from enum import Enum
from multiprocessing import JoinableQueue, Process, Queue
from pathlib import Path
from typing import Dict, Iterable, List, Optional

# Piper phonemization and utility imports
from piper_phonemize import (
    phonemize_espeak,
    phoneme_ids_espeak,
    get_espeak_map,
    get_max_phonemes,
    tashkeel_run,
)

# Local project imports
from .norm_audio import cache_norm_audio, make_silence_detector
from piper_train.symbols import _bos, _eos, _pad, symbols

# --- Global Constants ---
_DIR = Path(__file__).parent
_VERSION = (_DIR / "VERSION").read_text(encoding="utf-8").strip()
_LOGGER = logging.getLogger("preprocess")

# Create symbol-to-ID and ID-to-symbol maps from the project's symbol list
_symbol_to_id = {s: i for i, s in enumerate(symbols)}
_id_to_symbol = {i: s for i, s in enumerate(symbols)}


class PhonemeType(str, Enum):
    """Enumeration for the type of phoneme generation to use."""
    ESPEAK = "espeak"
    """Phonemes are generated by eSpeak-NG."""
    TEXT = "text"
    """Characters from the text itself are used as phonemes."""


class PathEncoder(json.JSONEncoder):
    """A JSON encoder that converts Path objects to strings."""
    def default(self, o):
        if isinstance(o, Path):
            return str(o)
        return super().default(o)


@dataclass
class Utterance:
    """Represents a single audio utterance and its metadata."""
    text: str
    audio_path: Path
    speaker: Optional[str] = None
    speaker_id: Optional[int] = None
    phonemes: Optional[List[str]] = None
    phoneme_ids: Optional[List[int]] = None
    audio_norm_path: Optional[Path] = None
    audio_spec_path: Optional[Path] = None
    missing_phonemes: "Counter[str]" = field(default_factory=Counter)


def main() -> None:
    """Main entry point for the data preprocessing script."""
    parser = argparse.ArgumentParser(description="Piper TTS data preprocessing script.")
    parser.add_argument("--input-dir", required=True, help="Directory with the audio dataset.")
    parser.add_argument("--output-dir", required=True, help="Directory to write training files.")
    parser.add_argument("--language", required=True, help="eSpeak-NG voice/language code (e.g., en-us).")
    parser.add_argument("--sample-rate", type=int, required=True, help="Target audio sample rate in Hz.")
    parser.add_argument("--dataset-format", choices=("ljspeech", "mycroft"), required=True, help="Format of the input dataset.")
    parser.add_argument("--cache-dir", help="Directory to cache processed audio files.")
    parser.add_argument("--max-workers", type=int, help="Maximum number of worker processes (defaults to CPU count).")
    parser.add_argument("--single-speaker", action="store_true", help="Force dataset to be treated as single-speaker.")
    parser.add_argument("--speaker-id", type=int, help="Assign a specific speaker ID for single-speaker datasets.")
    parser.add_argument("--phoneme-type", choices=list(PhonemeType), default=PhonemeType.ESPEAK, help="Type of phonemes to use (default: espeak).")
    parser.add_argument("--text-casing", choices=("ignore", "lower", "upper", "casefold"), default="ignore", help="Casing to apply to utterance text.")
    parser.add_argument("--dataset-name", help="Name of the dataset for the config file.")
    parser.add_argument("--audio-quality", help="Audio quality description for the config file.")
    parser.add_argument("--tashkeel", action="store_true", help="Diacritize Arabic text with libtashkeel.")
    parser.add_argument("--skip-audio", action="store_true", help="Skip audio processing steps.")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging.")
    args = parser.parse_args()

    # --- Initial Setup ---
    setup_logging(args.debug)
    
    # Validate arguments
    if args.single_speaker and (args.speaker_id is not None):
        _LOGGER.critical("--single-speaker and --speaker-id cannot be used together.")
        return

    # Prepare paths and directories
    args.phoneme_type = PhonemeType(args.phoneme_type)
    args.input_dir = Path(args.input_dir)
    args.output_dir = Path(args.output_dir)
    args.output_dir.mkdir(parents=True, exist_ok=True)
    args.cache_dir = Path(args.cache_dir) if args.cache_dir else args.output_dir / "cache"
    args.cache_dir.mkdir(parents=True, exist_ok=True)

    dataset_loader = mycroft_dataset if args.dataset_format == "mycroft" else ljspeech_dataset

    # --- Speaker and Utterance Analysis ---
    _LOGGER.info("Analyzing dataset to count speakers and utterances...")
    speaker_counts: "Counter[str]" = Counter()
    utterances = list(dataset_loader(args))
    num_utterances = len(utterances)

    if num_utterances == 0:
        _LOGGER.critical("No utterances found in the dataset. Exiting.")
        return

    for utt in utterances:
        speaker_counts[utt.speaker or ""] += 1
    
    is_multispeaker = len(speaker_counts) > 1
    speaker_ids: Dict[str, int] = {}
    if is_multispeaker:
        _LOGGER.info("%s speakers detected.", len(speaker_counts))
        # Assign speaker IDs sorted by utterance count (most frequent first)
        for speaker_id, (speaker, _) in enumerate(speaker_counts.most_common()):
            speaker_ids[speaker] = speaker_id
    else:
        _LOGGER.info("Single speaker dataset detected.")

    # --- Generate Config File ---
    write_config_file(args, speaker_counts, speaker_ids)

    # --- Process Data in Parallel ---
    process_utterances(args, utterances, speaker_ids, num_utterances)


def setup_logging(debug: bool):
    """Configures the logging for the script."""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(level=level, format="%(levelname)s: %(message)s")
    logging.getLogger().setLevel(level)
    logging.getLogger("numba").setLevel(logging.WARNING)  # Reduce log spam

def write_config_file(args: argparse.Namespace, speaker_counts: "Counter[str]", speaker_ids: Dict[str, int]):
    """Creates and writes the dataset's config.json file."""
    config_path = args.output_dir / "config.json"
    dataset_name = args.dataset_name or args.output_dir.parent.name
    audio_quality = args.audio_quality or args.output_dir.name
    
    config_data = {
        "dataset": dataset_name,
        "audio": {"sample_rate": args.sample_rate, "quality": audio_quality},
        "espeak": {"voice": args.language},
        "language": {"code": args.language},
        "inference": {"noise_scale": 0.667, "length_scale": 1, "noise_w": 0.8},
        "phoneme_type": args.phoneme_type.value,
        "phoneme_map": {},
        "phoneme_id_map": get_espeak_map() if args.phoneme_type == PhonemeType.ESPEAK else _symbol_to_id,
        "num_symbols": len(_symbol_to_id),
        "num_speakers": len(speaker_counts),
        "speaker_id_map": speaker_ids,
        "piper_version": _VERSION,
    }

    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config_data, f, ensure_ascii=False, indent=4)
    _LOGGER.info("Wrote dataset config to %s", config_path)


def process_utterances(args: argparse.Namespace, utterances: List[Utterance], speaker_ids: Dict[str, int], num_utterances: int):
    """Sets up multiprocessing and processes all utterances."""
    max_workers = args.max_workers or os.cpu_count() or 1
    batch_size = max(1, int(num_utterances / (max_workers * 2)))

    queue_in: "JoinableQueue[Iterable[Utterance]]" = JoinableQueue()
    queue_out: "Queue[Optional[Utterance]]" = Queue()

    # Determine which phonemizer function to use
    phonemizer_target = (
        phonemize_batch_espeak if args.phoneme_type == PhonemeType.ESPEAK else phonemize_batch_text
    )
    
    # Start worker processes
    processes = [
        Process(target=phonemizer_target, args=(args, queue_in, queue_out))
        for _ in range(max_workers)
    ]
    for proc in processes:
        proc.start()

    _LOGGER.info("Processing %s utterance(s) with %s worker(s)...", num_utterances, max_workers)

    # Feed utterance batches to workers
    for utt_batch in batched(utterances, batch_size):
        queue_in.put(utt_batch)

    # Collect results and write to dataset file
    missing_phonemes: "Counter[str]" = Counter()
    dataset_path = args.output_dir / "dataset.jsonl"
    with open(dataset_path, "w", encoding="utf-8") as dataset_file:
        for _ in range(num_utterances):
            utt = queue_out.get()
            if utt:
                if utt.speaker is not None:
                    utt.speaker_id = speaker_ids.get(utt.speaker)
                
                utt_dict = dataclasses.asdict(utt)
                utt_dict.pop("missing_phonemes", None)

                json.dump(utt_dict, dataset_file, ensure_ascii=False, cls=PathEncoder)
                dataset_file.write("\n")

                missing_phonemes.update(utt.missing_phonemes)

    if missing_phonemes:
        _LOGGER.warning("Found phonemes not in the symbol map:")
        for phoneme, count in missing_phonemes.most_common():
            _LOGGER.warning("Missing '%s' (%s times)", phoneme, count)

    _LOGGER.info("Finished processing. Wrote dataset to %s", dataset_path)

    # Stop worker processes
    for _ in range(len(processes)):
        queue_in.put(None)
    for proc in processes:
        proc.join(timeout=1)


def phonemize_batch_text(
    args: argparse.Namespace, queue_in: JoinableQueue, queue_out: Queue
):
    try:
        casing = get_text_casing(args.text_casing)
        silence_detector = make_silence_detector()

        while True:
            utt_batch = queue_in.get()
            if utt_batch is None:
                break

            for utt in utt_batch:
                try:
                    if args.tashkeel:
                        utt.text = tashkeel_run(utt.text)

                    _LOGGER.debug(utt)
                    
                    utt.text = casing(utt.text)
                    all_phonemes = utt.text
                    # Flatten
                    utt.phonemes = list(itertools.chain.from_iterable(all_phonemes))
                    utt.phoneme_ids = convert_phonemes_to_ids(utt.phonemes, _symbol_to_id, utt.missing_phonemes)
                    if not args.skip_audio:
                        utt.audio_norm_path, utt.audio_spec_path = cache_norm_audio(
                            utt.audio_path,
                            args.cache_dir,
                            silence_detector,
                            args.sample_rate,
                        )
                    queue_out.put(utt)
                except TimeoutError:
                    _LOGGER.error("Skipping utterance due to timeout: %s", utt)
                except Exception:
                    _LOGGER.exception("Failed to process utterance: %s", utt)
                    queue_out.put(None)

            queue_in.task_done()
    except Exception:
        _LOGGER.exception("phonemize_batch_text")
        

def phonemize_batch_espeak(args: argparse.Namespace, queue_in: JoinableQueue, queue_out: Queue):
    """Worker function to phonemize a batch of utterances using the espeak-ng command-line tool."""
    try:
        casing = get_text_casing(args.text_casing)
        silence_detector = make_silence_detector()

        while True:
            utt_batch = queue_in.get()
            if utt_batch is None:
                break

            for utt in utt_batch:
                try:
                    if args.tashkeel:
                        utt.text = tashkeel_run(utt.text)
                    
                    processed_text = casing(utt.text.strip())
                    _LOGGER.debug(utt)

                    all_phonemes = generate_phonemes_espeak_cli(processed_text, args.language)
                    if all_phonemes is None:
                        raise ValueError("Phoneme generation failed.")

                    # Flatten the list of lists into a single list of phonemes
                    utt.phonemes = list(itertools.chain.from_iterable(all_phonemes))
                    utt.phoneme_ids = convert_phonemes_to_ids(utt.phonemes, _symbol_to_id, utt.missing_phonemes)

                    if not args.skip_audio:
                        utt.audio_norm_path, utt.audio_spec_path = cache_norm_audio(
                            utt.audio_path, args.cache_dir, silence_detector, args.sample_rate
                        )
                    queue_out.put(utt)
                except Exception as e:
                    _LOGGER.error("Failed to process utterance: %s. Error: %s", utt, e)
                    queue_out.put(None)
            queue_in.task_done()
    except Exception as e:
        _LOGGER.exception("Critical error in phonemizer worker (text_cli): %s", e)


def generate_phonemes_espeak_cli(text: str, language: str) -> Optional[List[List[str]]]:
    """
    Generates phonemes for a given text using the espeak-ng command-line tool.

    Args:
        text: The sentence to generate phonemes for.
        language: The eSpeak-ng language/voice code.

    Returns:
        A list of lists of phoneme characters, or None on error.
    """
    try:
        command = ["espeak-ng", f"-v{language}", "-q", "-x", "--pho", text]
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        # Split phonemes by line and then each line into characters
        return [list(line) for line in result.stdout.strip().split("\n")]
    except subprocess.CalledProcessError as e:
        _LOGGER.error("eSpeak-ng failed for text '%s': %s", text, e)
        _LOGGER.error("Stderr: %s", e.stderr)
        return None
    except FileNotFoundError:
        _LOGGER.error("eSpeak-ng command not found. Ensure it is installed and in your system's PATH.")
        return None
    except Exception as e:
        _LOGGER.error("An unexpected error occurred during phoneme generation: %s", e)
        return None


def convert_phonemes_to_ids(
    phonemes: List[str], phoneme_id_map: Dict[str, int], missing_phonemes_counter: "Counter[str]"
) -> List[int]:
    """
    Converts a list of phonemes to a list of IDs using the provided map.
    It prepends a BOS token, appends an EOS token, and intersperses PAD tokens.
    
    Args:
        phonemes: A list of phoneme characters.
        phoneme_id_map: A dictionary mapping phonemes to their IDs.
        missing_phonemes_counter: A Counter to track phonemes not in the map.

    Returns:
        A list of phoneme IDs ready for the model.
    """
    bos_id = phoneme_id_map.get(_bos)
    eos_id = phoneme_id_map.get(_eos)
    pad_id = phoneme_id_map.get(_pad)

    if any(v is None for v in [bos_id, eos_id, pad_id]):
        raise ValueError("BOS, EOS, or PAD symbols are not in the phoneme_id_map.")

    phoneme_ids: List[int] = [bos_id, pad_id]
    for i, phoneme in enumerate(phonemes):
        phoneme_id = phoneme_id_map.get(phoneme)
        if phoneme_id is not None:
            phoneme_ids.append(phoneme_id)
        else:
            missing_phonemes_counter.update([phoneme])
            _LOGGER.warning("Phoneme '%s' not found in symbol map. Skipping.", phoneme)
        
        # Add a PAD token between phonemes
        if i < len(phonemes) - 1:
            phoneme_ids.append(pad_id)
    
    phoneme_ids.extend([pad_id, eos_id])
    return phoneme_ids


def get_text_casing(casing: str):
    """Returns a function to apply the specified text casing."""
    return {
        "lower": str.lower,
        "upper": str.upper,
        "casefold": str.casefold,
    }.get(casing, lambda s: s)


def ljspeech_dataset(args: argparse.Namespace) -> Iterable[Utterance]:
    """Loads utterances from a LJSpeech-formatted dataset."""
    dataset_dir = args.input_dir
    metadata_path = dataset_dir / "metadata.csv"
    if not metadata_path.exists():
        _LOGGER.error("Metadata file not found: %s", metadata_path)
        return

    wav_dir = dataset_dir / "wavs"
    if not wav_dir.is_dir():
        wav_dir = dataset_dir / "wav"

    with open(metadata_path, "r", encoding="utf-8") as csv_file:
        reader = csv.reader(csv_file, delimiter="|")
        for row in reader:
            if len(row) < 2:
                continue

            speaker: Optional[str] = None
            if args.single_speaker or len(row) == 2:
                filename, text = row[0], row[-1]
            else:
                filename, speaker, text = row[0], row[1], row[-1]

            # Find audio file
            possible_paths = [
                metadata_path.parent / f"{filename}.wav",
                wav_dir / f"{filename}.wav",
                metadata_path.parent / filename,
                wav_dir / filename,
            ]
            wav_path = next((p for p in possible_paths if p.exists()), None)
            
            if not args.skip_audio:
                if wav_path is None:
                    _LOGGER.warning("Audio file not found for: %s", filename)
                    continue
                if wav_path.stat().st_size == 0:
                    _LOGGER.warning("Audio file is empty: %s", wav_path)
                    continue
            
            yield Utterance(
                text=text,
                audio_path=wav_path or Path(filename),
                speaker=speaker,
                speaker_id=args.speaker_id,
            )


def mycroft_dataset(args: argparse.Namespace) -> Iterable[Utterance]:
    """Loads utterances from a Mycroft-formatted dataset."""
    for metadata_path in args.input_dir.glob("**/dialog/metadata.csv"):
        speaker = metadata_path.parent.parent.name if not args.single_speaker else None
        with open(metadata_path, "r", encoding="utf-8") as csv_file:
            reader = csv.reader(csv_file, delimiter="|")
            for row in reader:
                if len(row) < 2:
                    continue
                filename, text = row[0], row[1]
                wav_path = metadata_path.parent / f"{filename}.wav"
                
                if args.skip_audio or (wav_path.exists() and wav_path.stat().st_size > 0):
                    yield Utterance(
                        text=text,
                        audio_path=wav_path,
                        speaker=speaker
                    )


def batched(iterable, n: int):
    """Batch data into lists of length n. The last batch may be shorter."""
    if n < 1:
        raise ValueError("n must be at least one")
    it = iter(iterable)
    while batch := list(itertools.islice(it, n)):
        yield batch


if __name__ == "__main__":
    main()